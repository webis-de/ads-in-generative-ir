import hashlib
from langdetect import detect
import nltk
from nltk.tokenize import sent_tokenize
import pandas as pd
import re
from tqdm import tqdm

from ads_in_generative_ir import RESOURCE_PATH
nltk.download('punkt')

class ResponseFilter:
    def __init__(self, meta_topic: str = "workout"):
        self.meta_topic = meta_topic
        self.df = pd.read_csv(RESOURCE_PATH / f'generated_responses/{meta_topic}_responses.csv')
        with open(RESOURCE_PATH / f'queries/{meta_topic}_queries.txt', 'r') as file:
            self.queries = [q.replace("\n", "") for q in file.readlines()]

    def filter_df(self, lang: str = None, top_queries: int = None):
        lang = lang or "en"
        min_sen = 4
        max_sen = 12
        top_queries = top_queries or 500

        print(f"Filtering {self.meta_topic} responses (lang={lang}, "
              f"min_sen={min_sen}, max_sen={max_sen}, top_queries={top_queries})")
        tqdm.pandas(desc=f"Processing rows")
        mask = self.df.progress_apply(lambda row: self.filter_row(row, lang, min_sen, max_sen, top_queries), axis=1)

        out_path = RESOURCE_PATH / f'generated_responses/{self.meta_topic}_filtered.csv'
        filtered_df = self.df.loc[mask]

        # Remove any sources at the end of sentences in the responses as these can cause issues with the injection
        filtered_df = filtered_df.apply(lambda row: self._clean_response_row(row), axis=1)

        # Add an id column and write the results
        # - 4 digits for hash of meta-topic
        # - 6 digit incrementing number
        # - "N" for "no ad"
        topic_hash = int(hashlib.sha1(self.meta_topic.encode("utf-8")).hexdigest(), 16) % (10 ** 4)
        filtered_df.insert(loc=0,
                           column='response_id',
                           value=[f"{topic_hash}-{format(i, '06d')}-N" for i in range(len(filtered_df))])
        filtered_df.to_csv(out_path, index=False)
        print("\n")
        print(f"Kept {filtered_df.shape[0]} responses. Saved result to {out_path}")

    def filter_row(self, row, lang, min_sen, max_sen, top_queries) -> bool:
        return (self.is_lang(row["response"], lang)
                & self.in_sentences_range(row["response"], min_sen=min_sen, max_sen=max_sen)
                & self.is_top_k_query(row["query"], self.queries, top_queries))

    @staticmethod
    def is_lang(response: str, lang: str = "en") -> bool:
        try:
            return detect(response) == lang
        except:
            return False

    @staticmethod
    def in_sentences_range(response: str, min_sen: int = 4, max_sen: int = 8) -> bool:
        return min_sen <= len(sent_tokenize(response)) <= max_sen

    @staticmethod
    def is_top_k_query(query: str, queries, k: int = 500) -> bool:
        return query in queries[:k]

    def _clean_response_row(self, row):
        num_sources = len(eval(row["sources"]))
        if num_sources > 0:
            row["response"] = self._remove_sources_from_response(row["response"], num_sources)
        return row

    @staticmethod
    def _remove_sources_from_response(s, num_sources=3):
        # Replace any list of sources (separated by comma, whitespace, newline, ... or " and ");
        source_list = r"[1-" + re.escape(str(num_sources)) + r"]"
        sep_in_source_list = r"([\s\t\r\n]|\,\s|\sand\s)"
        sep_1st_source = r"[\s\t\r\n]"

        optional_1st_block = r"(" + sep_1st_source + source_list + r")?"
        optional_source_block = r"(" + sep_in_source_list + source_list + r")?"

        # Case 1: End of sentence (Optional comma at the end to catch edge cases)
        # Catches up to three consecutive sources
        # Ensures that there is no colon in the next 20 chars after the number to not match enumerations
        source_followed_by_punct = sep_in_source_list + source_list + r"[\s\t\r\n]?\,?\.(?!(?:(?!\.).){0,20}\:)"
        pattern1 = (optional_1st_block + optional_source_block + optional_source_block + source_followed_by_punct)
        s_tmp = re.sub(pattern1, '.', s)

        # Case 2: After the end of a sentence, followed by newline
        punct_followed_by_source = r"[\s\t\r\n]?" + source_list + r"\n"
        pattern2 = r"\." + optional_source_block + r"\s?" + optional_source_block + punct_followed_by_source
        s_tmp = re.sub(pattern2, '. \n', s_tmp)

        # Case 3: Sources in between
        pattern3 = r"\n(" + source_list + r"\n?)+\n"

        # Apply both patterns and return
        return re.sub(pattern3, ' ', s_tmp)

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(
        prog='Response Filter',
        description='Apply filtering conditions to the responses generated by retrieval-augmented chatbots')

    parser.add_argument("meta_topic", metavar="M", type=str, help="Meta topic for which to filter responses")
    parser.add_argument('-l', '--lang', type=str, help='Language to filter responses for (default "en")')
    parser.add_argument('-q', '--queries', type=int, help='Top k queries for which to keep responses (default 500)')

    args = parser.parse_args()

    r_filter = ResponseFilter(args.meta_topic)
    r_filter.filter_df(lang=args.lang, top_queries=args.queries)